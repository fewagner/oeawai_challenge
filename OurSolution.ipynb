{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:07.864485Z",
     "start_time": "2019-08-12T05:02:07.253464Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "from IPython.display import Audio\n",
    "import torch.optim as optim\n",
    "from types import SimpleNamespace\n",
    "import scipy.signal as sc\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "from trainDataset import TrainDataset\n",
    "from testDataset import TestDataset\n",
    "#from trainDatasetNew import TrainDatasetNew\n",
    "#from testDatasetNew import TestDatasetNew\n",
    "from validation_split import get_dataloaders\n",
    "from math_utils import logMagStft, ffts\n",
    "from SpectrogramCNN import SpectrogramCNN\n",
    "from train_utils import train, test, save_output, save_geometric_mean_predictions\n",
    "from evaluation_utils import get_mean_F1\n",
    "from MulitScale1DResNet import MSResNet\n",
    "from LSTM import LSTM\n",
    "from FFTMulitScale1DResNet import FFTMSResNet\n",
    "from SpectralResNet import SpectralResNet34\n",
    "from scipy.signal import hilbert\n",
    "from FeatureFNN import FeatureFNN\n",
    "from MelRawCombined import CNN1D, CNN2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:07.892800Z",
     "start_time": "2019-08-12T05:02:07.866328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with cuda\n"
     ]
    }
   ],
   "source": [
    "validation_split = .2\n",
    "do_plots = False\n",
    "load_model = True\n",
    "args = SimpleNamespace(batch_size=64, test_batch_size=64, epochs=7,\n",
    "                       lr=0.001, momentum=0.5, seed=1, log_interval=200, \n",
    "                      net = CNN2D) #SpectrogramCNN, MSResNet, SpectralResNet34, FFTMSResNet, FeatureFNN, CNN1D, CNN2D \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not torch.cuda.is_available(): # adapt those paths on other machine\n",
    "    print('no cuda')\n",
    "    path_train = './../data/train-small/'\n",
    "    path_test =  './../data/test/kaggle-test/'\n",
    "else:\n",
    "    print('with cuda')\n",
    "    path_train = './../data/kaggle-train/'\n",
    "    path_test =  './../data/kaggle-test/'\n",
    "    \n",
    "path_model = 'models/model.pt'\n",
    "path_submission = 'submissions/'\n",
    "    \n",
    "sample_rate = 16000\n",
    "nmbr_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.173618Z",
     "start_time": "2019-08-12T05:02:07.894841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269776\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "# todo add in the classes the features and the fft data\n",
    "\n",
    "toFloat = transforms.Lambda(lambda x: x / np.iinfo(np.int16).max)\n",
    "\n",
    "trainDataset = TrainDataset(path_train, transform=toFloat)\n",
    "print(len(trainDataset))\n",
    "\n",
    "testDataset = TestDataset(path_test, transform=toFloat)\n",
    "print(len(testDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.185671Z",
     "start_time": "2019-08-12T05:02:09.178228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  64000\n"
     ]
    }
   ],
   "source": [
    "input_size = len(trainDataset[0][0])\n",
    "print('input size: ',input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Look at Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.248890Z",
     "start_time": "2019-08-12T05:02:09.187726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if do_plots:\n",
    "    # how many instruments are there?\n",
    "    dummy_count = np.zeros(20)\n",
    "\n",
    "    for sample in trainDataset:\n",
    "        dummy_count[sample[1]] += 1\n",
    "\n",
    "    labels_count = []\n",
    "    for elem in dummy_count:\n",
    "        if elem != 0:\n",
    "            labels_count.append(elem)\n",
    "\n",
    "    print(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.254748Z",
     "start_time": "2019-08-12T05:02:09.252404Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if do_plots:\n",
    "    nmbr_classes = len(labels_count)\n",
    "    print('nmbr_classes: ', nmbr_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.259734Z",
     "start_time": "2019-08-12T05:02:09.257212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if do_plots:\n",
    "    plt.plot(labels_count, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.264167Z",
     "start_time": "2019-08-12T05:02:09.261572Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if do_plots:\n",
    "    # plot one of each\n",
    "\n",
    "    done = np.zeros(nmbr_classes)\n",
    "    examples = []\n",
    "\n",
    "    for sample in trainDataset:\n",
    "        if done[sample[1]] == 0:\n",
    "            examples.append(sample)\n",
    "            done[sample[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.267657Z",
     "start_time": "2019-08-12T05:02:09.265650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.271267Z",
     "start_time": "2019-08-12T05:02:09.268935Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#time series\n",
    "\n",
    "if do_plots:\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        plt.subplot(4,3,i+1)\n",
    "        plt.plot(examples[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.276048Z",
     "start_time": "2019-08-12T05:02:09.272375Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#envelope\n",
    "\n",
    "if do_plots:\n",
    "    \n",
    "    m = nn.MaxPool1d(50)\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        plt.subplot(4,3,i+1)\n",
    "        env = torch.from_numpy(np.abs(hilbert(examples[i][0]))).view(1,1,-1)\n",
    "        env = m(env)\n",
    "        plt.plot(env[0,0].numpy())\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.281884Z",
     "start_time": "2019-08-12T05:02:09.277342Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fft\n",
    "\n",
    "if do_plots:\n",
    "\n",
    "    m = nn.MaxPool1d(50)\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        plt.subplot(4,3,i+1)\n",
    "        fft = torch.from_numpy(ffts(examples[i][0])).view(1,1,-1)\n",
    "        fft = m(fft)\n",
    "        plt.plot(fft[0,0].numpy())\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.287149Z",
     "start_time": "2019-08-12T05:02:09.283554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# melspec\n",
    "\n",
    "if do_plots:\n",
    "    n_fft = 510\n",
    "    for i in range(len(examples)):\n",
    "        plt.subplot(4,3,i+1)\n",
    "        spectrogram = logMagStft(examples[i][0], 16000, n_fft)\n",
    "        plt.imshow(spectrogram)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.291377Z",
     "start_time": "2019-08-12T05:02:09.288687Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if do_plots:\n",
    "\n",
    "    for sample in examples:\n",
    "        display(Audio(sample[0], rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.458518Z",
     "start_time": "2019-08-12T05:02:09.292524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64000]) torch.Size([64]) tensor([0, 3, 4, 3, 6, 6, 9, 5, 3, 3, 5, 6, 8, 7, 2, 4, 3, 1, 4, 8, 4, 5, 1, 3,\n",
      "        7, 6, 5, 4, 8, 5, 6, 4, 0, 0, 7, 7, 8, 8, 4, 7, 3, 0, 0, 7, 0, 6, 6, 4,\n",
      "        4, 0, 0, 0, 5, 6, 6, 9, 5, 0, 8, 0, 8, 4, 3, 0])\n",
      "tensor(-0.9551, dtype=torch.float64) tensor(0.9466, dtype=torch.float64)\n",
      "['bass' 'guitar' 'keyboard' 'guitar' 'organ' 'organ' 'vocal' 'mallet'\n",
      " 'guitar' 'guitar' 'mallet' 'organ' 'string' 'reed' 'flute' 'keyboard'\n",
      " 'guitar' 'brass' 'keyboard' 'string' 'keyboard' 'mallet' 'brass' 'guitar'\n",
      " 'reed' 'organ' 'mallet' 'keyboard' 'string' 'mallet' 'organ' 'keyboard'\n",
      " 'bass' 'bass' 'reed' 'reed' 'string' 'string' 'keyboard' 'reed' 'guitar'\n",
      " 'bass' 'bass' 'reed' 'bass' 'organ' 'organ' 'keyboard' 'keyboard' 'bass'\n",
      " 'bass' 'bass' 'mallet' 'organ' 'organ' 'vocal' 'mallet' 'bass' 'string'\n",
      " 'bass' 'string' 'keyboard' 'guitar' 'bass']\n",
      "torch.Size([64, 64000]) torch.Size([64]) tensor([9, 3, 5, 0, 8, 6, 3, 0, 4, 0, 8, 8, 0, 4, 3, 0, 3, 3, 0, 4, 0, 5, 6, 4,\n",
      "        3, 0, 8, 3, 8, 2, 4, 9, 4, 4, 3, 1, 8, 4, 4, 4, 1, 8, 4, 4, 0, 4, 4, 6,\n",
      "        3, 0, 5, 6, 4, 6, 4, 6, 7, 3, 6, 3, 6, 0, 0, 5])\n",
      "tensor(-0.9521, dtype=torch.float64) tensor(0.9559, dtype=torch.float64)\n",
      "['vocal' 'guitar' 'mallet' 'bass' 'string' 'organ' 'guitar' 'bass'\n",
      " 'keyboard' 'bass' 'string' 'string' 'bass' 'keyboard' 'guitar' 'bass'\n",
      " 'guitar' 'guitar' 'bass' 'keyboard' 'bass' 'mallet' 'organ' 'keyboard'\n",
      " 'guitar' 'bass' 'string' 'guitar' 'string' 'flute' 'keyboard' 'vocal'\n",
      " 'keyboard' 'keyboard' 'guitar' 'brass' 'string' 'keyboard' 'keyboard'\n",
      " 'keyboard' 'brass' 'string' 'keyboard' 'keyboard' 'bass' 'keyboard'\n",
      " 'keyboard' 'organ' 'guitar' 'bass' 'mallet' 'organ' 'keyboard' 'organ'\n",
      " 'keyboard' 'organ' 'reed' 'guitar' 'organ' 'guitar' 'organ' 'bass' 'bass'\n",
      " 'mallet']\n"
     ]
    }
   ],
   "source": [
    "# validation split is done here\n",
    "\n",
    "train_loader, validation_loader, indices = get_dataloaders(trainDataset, \n",
    "                                                  batch_size = args.batch_size, \n",
    "                                                  validation_split = validation_split, \n",
    "                                                  shuffle_dataset = True, \n",
    "                                                  random_seed = None)\n",
    "                                                    #indices = indices)\n",
    "\n",
    "for samples, instrument_family_target in train_loader:\n",
    "        print(samples.shape, instrument_family_target.shape,\n",
    "              instrument_family_target.data)\n",
    "        print(torch.min(samples), torch.max(samples))\n",
    "        print(trainDataset.transformInstrumentsFamilyToString(instrument_family_target.data))\n",
    "        break\n",
    "        \n",
    "for samples, instrument_family_target in validation_loader:\n",
    "        print(samples.shape, instrument_family_target.shape,\n",
    "              instrument_family_target.data)\n",
    "        print(torch.min(samples), torch.max(samples))\n",
    "        print(trainDataset.transformInstrumentsFamilyToString(instrument_family_target.data))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.481581Z",
     "start_time": "2019-08-12T05:02:09.459998Z"
    }
   },
   "outputs": [],
   "source": [
    "# save indices\n",
    "with open(path_submission + \"indices.txt\", \"wb\") as fp:\n",
    "    pickle.dump([indices], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:09.530394Z",
     "start_time": "2019-08-12T05:02:09.483120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64000])\n",
      "tensor(-0.9723, dtype=torch.float64) tensor(0.9834, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_loader = data.DataLoader(testDataset, batch_size=args.batch_size, shuffle=False) #!!! shuffle should be false\n",
    "for samples in test_loader:\n",
    "        print(samples[0].shape)\n",
    "        print(torch.min(samples[0]), torch.max(samples[0]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T02:22:21.916354Z",
     "start_time": "2019-08-12T02:22:19.695504Z"
    }
   },
   "outputs": [],
   "source": [
    "if (args.net == SpectrogramCNN) or (args.net == SpectralResNet34) \\\n",
    "    or (args.net == FeatureFNN) or (args.net == CNN1D) \\\n",
    "    or (args.net == CNN2D):\n",
    "    model = args.net(device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T02:22:21.920588Z",
     "start_time": "2019-08-12T02:22:21.917839Z"
    }
   },
   "outputs": [],
   "source": [
    "if (args.net == MSResNet) or (args.net == FFTMSResNet):\n",
    "    model = args.net(1, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T02:22:21.926369Z",
     "start_time": "2019-08-12T02:22:21.922627Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.net == LSTM:\n",
    "    model = args.net(device, input_size = 252, hidden_size = 512, num_layers = 1, num_classes = 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T02:22:21.932056Z",
     "start_time": "2019-08-12T02:22:21.928412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN2D(\n",
      "  (conv_1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 10), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_2): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(4, 10), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_3): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(4, 10), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_4): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(4, 10), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2912, out_features=64, bias=True)\n",
      "  (batchnorm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T02:22:21.937435Z",
     "start_time": "2019-08-12T02:22:21.934603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "info = {'highest F1' : 0,\n",
    "        'saved epoch' : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T04:35:55.538744Z",
     "start_time": "2019-08-12T02:22:21.940939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/269776 (0%)]\tLoss: 2.303088\tF1: 0.0625\tRuntime: 0.7\n",
      "Train Epoch: 1 [12800/269776 (6%)]\tLoss: 1.333126\tF1: 0.5000\tRuntime: 56.6\n",
      "Train Epoch: 1 [25600/269776 (12%)]\tLoss: 1.117404\tF1: 0.6406\tRuntime: 112.5\n",
      "Train Epoch: 1 [38400/269776 (18%)]\tLoss: 1.005882\tF1: 0.6719\tRuntime: 168.2\n",
      "Train Epoch: 1 [51200/269776 (24%)]\tLoss: 0.947920\tF1: 0.6250\tRuntime: 224.0\n",
      "Train Epoch: 1 [64000/269776 (30%)]\tLoss: 0.718413\tF1: 0.7812\tRuntime: 279.7\n",
      "Train Epoch: 1 [76800/269776 (36%)]\tLoss: 0.641225\tF1: 0.7500\tRuntime: 335.5\n",
      "Train Epoch: 1 [89600/269776 (42%)]\tLoss: 0.520325\tF1: 0.8281\tRuntime: 391.2\n",
      "Train Epoch: 1 [102400/269776 (47%)]\tLoss: 0.676766\tF1: 0.7344\tRuntime: 447.1\n",
      "Train Epoch: 1 [115200/269776 (53%)]\tLoss: 0.665857\tF1: 0.7500\tRuntime: 502.8\n",
      "Train Epoch: 1 [128000/269776 (59%)]\tLoss: 0.421030\tF1: 0.8594\tRuntime: 558.5\n",
      "Train Epoch: 1 [140800/269776 (65%)]\tLoss: 0.653633\tF1: 0.7344\tRuntime: 614.3\n",
      "Train Epoch: 1 [153600/269776 (71%)]\tLoss: 0.803640\tF1: 0.7188\tRuntime: 669.9\n",
      "Train Epoch: 1 [166400/269776 (77%)]\tLoss: 0.315333\tF1: 0.9219\tRuntime: 725.7\n",
      "Train Epoch: 1 [179200/269776 (83%)]\tLoss: 0.476091\tF1: 0.8281\tRuntime: 781.4\n",
      "Train Epoch: 1 [192000/269776 (89%)]\tLoss: 0.312956\tF1: 0.8906\tRuntime: 837.3\n",
      "Train Epoch: 1 [204800/269776 (95%)]\tLoss: 0.544465\tF1: 0.8281\tRuntime: 893.0\n",
      "after epoch 1 got f1 score of 0.29602340047393416\n",
      "saved predictions\n",
      "currently best model --> saved\n",
      "Train Epoch: 2 [0/269776 (0%)]\tLoss: 0.621198\tF1: 0.8125\tRuntime: 0.4\n",
      "Train Epoch: 2 [12800/269776 (6%)]\tLoss: 0.307585\tF1: 0.8906\tRuntime: 56.1\n",
      "Train Epoch: 2 [25600/269776 (12%)]\tLoss: 0.373263\tF1: 0.9062\tRuntime: 111.9\n",
      "Train Epoch: 2 [38400/269776 (18%)]\tLoss: 0.222876\tF1: 0.9219\tRuntime: 167.8\n",
      "Train Epoch: 2 [51200/269776 (24%)]\tLoss: 0.338916\tF1: 0.8594\tRuntime: 223.6\n",
      "Train Epoch: 2 [64000/269776 (30%)]\tLoss: 0.257956\tF1: 0.9219\tRuntime: 279.4\n",
      "Train Epoch: 2 [76800/269776 (36%)]\tLoss: 0.323576\tF1: 0.9219\tRuntime: 335.2\n",
      "Train Epoch: 2 [89600/269776 (42%)]\tLoss: 0.182574\tF1: 0.9531\tRuntime: 391.1\n",
      "Train Epoch: 2 [102400/269776 (47%)]\tLoss: 0.194147\tF1: 0.9219\tRuntime: 447.1\n",
      "Train Epoch: 2 [115200/269776 (53%)]\tLoss: 0.213312\tF1: 0.9531\tRuntime: 502.9\n",
      "Train Epoch: 2 [128000/269776 (59%)]\tLoss: 0.260011\tF1: 0.9062\tRuntime: 558.7\n",
      "Train Epoch: 2 [140800/269776 (65%)]\tLoss: 0.197525\tF1: 0.9219\tRuntime: 614.5\n",
      "Train Epoch: 2 [153600/269776 (71%)]\tLoss: 0.313034\tF1: 0.8906\tRuntime: 670.2\n",
      "Train Epoch: 2 [166400/269776 (77%)]\tLoss: 0.168384\tF1: 0.9375\tRuntime: 726.0\n",
      "Train Epoch: 2 [179200/269776 (83%)]\tLoss: 0.154365\tF1: 0.9688\tRuntime: 781.9\n",
      "Train Epoch: 2 [192000/269776 (89%)]\tLoss: 0.180673\tF1: 0.9531\tRuntime: 837.6\n",
      "Train Epoch: 2 [204800/269776 (95%)]\tLoss: 0.304244\tF1: 0.9219\tRuntime: 893.5\n",
      "after epoch 2 got f1 score of 0.4195732128751985\n",
      "saved predictions\n",
      "currently best model --> saved\n",
      "Train Epoch: 3 [0/269776 (0%)]\tLoss: 0.259201\tF1: 0.9375\tRuntime: 0.4\n",
      "Train Epoch: 3 [12800/269776 (6%)]\tLoss: 0.101906\tF1: 0.9688\tRuntime: 56.3\n",
      "Train Epoch: 3 [25600/269776 (12%)]\tLoss: 0.248668\tF1: 0.8906\tRuntime: 112.2\n",
      "Train Epoch: 3 [38400/269776 (18%)]\tLoss: 0.197445\tF1: 0.8906\tRuntime: 168.2\n",
      "Train Epoch: 3 [51200/269776 (24%)]\tLoss: 0.178787\tF1: 0.9219\tRuntime: 224.2\n",
      "Train Epoch: 3 [64000/269776 (30%)]\tLoss: 0.380598\tF1: 0.8906\tRuntime: 280.2\n",
      "Train Epoch: 3 [76800/269776 (36%)]\tLoss: 0.092123\tF1: 0.9531\tRuntime: 336.1\n",
      "Train Epoch: 3 [89600/269776 (42%)]\tLoss: 0.244560\tF1: 0.9219\tRuntime: 391.8\n",
      "Train Epoch: 3 [102400/269776 (47%)]\tLoss: 0.114325\tF1: 0.9688\tRuntime: 447.5\n",
      "Train Epoch: 3 [115200/269776 (53%)]\tLoss: 0.120733\tF1: 0.9531\tRuntime: 503.5\n",
      "Train Epoch: 3 [128000/269776 (59%)]\tLoss: 0.129713\tF1: 0.9375\tRuntime: 559.5\n",
      "Train Epoch: 3 [140800/269776 (65%)]\tLoss: 0.061795\tF1: 0.9844\tRuntime: 615.4\n",
      "Train Epoch: 3 [153600/269776 (71%)]\tLoss: 0.218202\tF1: 0.9219\tRuntime: 671.4\n",
      "Train Epoch: 3 [166400/269776 (77%)]\tLoss: 0.127266\tF1: 0.9375\tRuntime: 727.2\n",
      "Train Epoch: 3 [179200/269776 (83%)]\tLoss: 0.394989\tF1: 0.9062\tRuntime: 783.2\n",
      "Train Epoch: 3 [192000/269776 (89%)]\tLoss: 0.068601\tF1: 0.9688\tRuntime: 839.2\n",
      "Train Epoch: 3 [204800/269776 (95%)]\tLoss: 0.137498\tF1: 0.9219\tRuntime: 895.1\n",
      "after epoch 3 got f1 score of 0.4875654127172207\n",
      "saved predictions\n",
      "currently best model --> saved\n",
      "Train Epoch: 4 [0/269776 (0%)]\tLoss: 0.091031\tF1: 0.9688\tRuntime: 0.4\n",
      "Train Epoch: 4 [12800/269776 (6%)]\tLoss: 0.259701\tF1: 0.9062\tRuntime: 56.1\n",
      "Train Epoch: 4 [25600/269776 (12%)]\tLoss: 0.171482\tF1: 0.9688\tRuntime: 112.0\n",
      "Train Epoch: 4 [38400/269776 (18%)]\tLoss: 0.137639\tF1: 0.9688\tRuntime: 167.7\n",
      "Train Epoch: 4 [51200/269776 (24%)]\tLoss: 0.167999\tF1: 0.9375\tRuntime: 223.5\n",
      "Train Epoch: 4 [64000/269776 (30%)]\tLoss: 0.100670\tF1: 0.9375\tRuntime: 279.5\n",
      "Train Epoch: 4 [76800/269776 (36%)]\tLoss: 0.067674\tF1: 0.9844\tRuntime: 335.3\n",
      "Train Epoch: 4 [89600/269776 (42%)]\tLoss: 0.053101\tF1: 0.9844\tRuntime: 391.1\n",
      "Train Epoch: 4 [102400/269776 (47%)]\tLoss: 0.258744\tF1: 0.9062\tRuntime: 447.0\n",
      "Train Epoch: 4 [115200/269776 (53%)]\tLoss: 0.080157\tF1: 0.9688\tRuntime: 502.8\n",
      "Train Epoch: 4 [128000/269776 (59%)]\tLoss: 0.090483\tF1: 0.9531\tRuntime: 558.6\n",
      "Train Epoch: 4 [140800/269776 (65%)]\tLoss: 0.031028\tF1: 1.0000\tRuntime: 614.6\n",
      "Train Epoch: 4 [153600/269776 (71%)]\tLoss: 0.099557\tF1: 0.9688\tRuntime: 670.6\n",
      "Train Epoch: 4 [166400/269776 (77%)]\tLoss: 0.133203\tF1: 0.9375\tRuntime: 726.5\n",
      "Train Epoch: 4 [179200/269776 (83%)]\tLoss: 0.069563\tF1: 0.9688\tRuntime: 782.4\n",
      "Train Epoch: 4 [192000/269776 (89%)]\tLoss: 0.037288\tF1: 1.0000\tRuntime: 838.3\n",
      "Train Epoch: 4 [204800/269776 (95%)]\tLoss: 0.142561\tF1: 0.9375\tRuntime: 894.2\n",
      "after epoch 4 got f1 score of 0.7488336789099577\n",
      "saved predictions\n",
      "currently best model --> saved\n",
      "Train Epoch: 5 [0/269776 (0%)]\tLoss: 0.098993\tF1: 0.9531\tRuntime: 0.4\n",
      "Train Epoch: 5 [12800/269776 (6%)]\tLoss: 0.010600\tF1: 1.0000\tRuntime: 56.2\n",
      "Train Epoch: 5 [25600/269776 (12%)]\tLoss: 0.041253\tF1: 1.0000\tRuntime: 112.1\n",
      "Train Epoch: 5 [38400/269776 (18%)]\tLoss: 0.131565\tF1: 0.9375\tRuntime: 168.1\n",
      "Train Epoch: 5 [51200/269776 (24%)]\tLoss: 0.133401\tF1: 0.9688\tRuntime: 224.0\n",
      "Train Epoch: 5 [64000/269776 (30%)]\tLoss: 0.143112\tF1: 0.9531\tRuntime: 279.9\n",
      "Train Epoch: 5 [76800/269776 (36%)]\tLoss: 0.036201\tF1: 0.9844\tRuntime: 335.8\n",
      "Train Epoch: 5 [89600/269776 (42%)]\tLoss: 0.106053\tF1: 0.9688\tRuntime: 391.7\n",
      "Train Epoch: 5 [102400/269776 (47%)]\tLoss: 0.079493\tF1: 0.9531\tRuntime: 447.7\n",
      "Train Epoch: 5 [115200/269776 (53%)]\tLoss: 0.088317\tF1: 0.9688\tRuntime: 503.7\n",
      "Train Epoch: 5 [128000/269776 (59%)]\tLoss: 0.155763\tF1: 0.9219\tRuntime: 559.8\n",
      "Train Epoch: 5 [140800/269776 (65%)]\tLoss: 0.054253\tF1: 0.9688\tRuntime: 615.8\n",
      "Train Epoch: 5 [153600/269776 (71%)]\tLoss: 0.076639\tF1: 0.9688\tRuntime: 671.7\n",
      "Train Epoch: 5 [166400/269776 (77%)]\tLoss: 0.050714\tF1: 0.9844\tRuntime: 727.7\n",
      "Train Epoch: 5 [179200/269776 (83%)]\tLoss: 0.101877\tF1: 0.9844\tRuntime: 783.6\n",
      "Train Epoch: 5 [192000/269776 (89%)]\tLoss: 0.112388\tF1: 0.9531\tRuntime: 839.6\n",
      "Train Epoch: 5 [204800/269776 (95%)]\tLoss: 0.016797\tF1: 1.0000\tRuntime: 895.6\n",
      "after epoch 5 got f1 score of 0.31992372630331833\n",
      "Train Epoch: 6 [0/269776 (0%)]\tLoss: 0.059219\tF1: 0.9844\tRuntime: 0.4\n",
      "Train Epoch: 6 [12800/269776 (6%)]\tLoss: 0.056890\tF1: 0.9844\tRuntime: 56.1\n",
      "Train Epoch: 6 [25600/269776 (12%)]\tLoss: 0.060425\tF1: 0.9688\tRuntime: 112.0\n",
      "Train Epoch: 6 [38400/269776 (18%)]\tLoss: 0.194932\tF1: 0.9375\tRuntime: 168.2\n",
      "Train Epoch: 6 [51200/269776 (24%)]\tLoss: 0.038654\tF1: 0.9844\tRuntime: 225.7\n",
      "Train Epoch: 6 [64000/269776 (30%)]\tLoss: 0.051114\tF1: 0.9688\tRuntime: 281.9\n",
      "Train Epoch: 6 [76800/269776 (36%)]\tLoss: 0.033288\tF1: 0.9844\tRuntime: 338.8\n",
      "Train Epoch: 6 [89600/269776 (42%)]\tLoss: 0.040766\tF1: 1.0000\tRuntime: 395.1\n",
      "Train Epoch: 6 [102400/269776 (47%)]\tLoss: 0.098556\tF1: 0.9844\tRuntime: 451.2\n",
      "Train Epoch: 6 [115200/269776 (53%)]\tLoss: 0.007362\tF1: 1.0000\tRuntime: 507.2\n",
      "Train Epoch: 6 [128000/269776 (59%)]\tLoss: 0.191284\tF1: 0.9531\tRuntime: 563.1\n",
      "Train Epoch: 6 [140800/269776 (65%)]\tLoss: 0.036997\tF1: 0.9844\tRuntime: 619.2\n",
      "Train Epoch: 6 [153600/269776 (71%)]\tLoss: 0.044921\tF1: 0.9844\tRuntime: 675.3\n",
      "Train Epoch: 6 [166400/269776 (77%)]\tLoss: 0.012854\tF1: 1.0000\tRuntime: 731.9\n",
      "Train Epoch: 6 [179200/269776 (83%)]\tLoss: 0.053168\tF1: 0.9844\tRuntime: 788.0\n",
      "Train Epoch: 6 [192000/269776 (89%)]\tLoss: 0.038866\tF1: 1.0000\tRuntime: 845.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [204800/269776 (95%)]\tLoss: 0.201684\tF1: 0.9219\tRuntime: 901.1\n",
      "after epoch 6 got f1 score of 0.33443794431279666\n",
      "Train Epoch: 7 [0/269776 (0%)]\tLoss: 0.024902\tF1: 1.0000\tRuntime: 0.4\n",
      "Train Epoch: 7 [12800/269776 (6%)]\tLoss: 0.027976\tF1: 1.0000\tRuntime: 56.5\n",
      "Train Epoch: 7 [25600/269776 (12%)]\tLoss: 0.098622\tF1: 0.9531\tRuntime: 112.6\n",
      "Train Epoch: 7 [38400/269776 (18%)]\tLoss: 0.094430\tF1: 0.9844\tRuntime: 168.6\n",
      "Train Epoch: 7 [51200/269776 (24%)]\tLoss: 0.089394\tF1: 0.9688\tRuntime: 224.6\n",
      "Train Epoch: 7 [64000/269776 (30%)]\tLoss: 0.015583\tF1: 1.0000\tRuntime: 280.6\n",
      "Train Epoch: 7 [76800/269776 (36%)]\tLoss: 0.043169\tF1: 0.9844\tRuntime: 336.5\n",
      "Train Epoch: 7 [89600/269776 (42%)]\tLoss: 0.058544\tF1: 0.9844\tRuntime: 392.5\n",
      "Train Epoch: 7 [102400/269776 (47%)]\tLoss: 0.022424\tF1: 1.0000\tRuntime: 448.4\n",
      "Train Epoch: 7 [115200/269776 (53%)]\tLoss: 0.027738\tF1: 0.9844\tRuntime: 505.3\n",
      "Train Epoch: 7 [128000/269776 (59%)]\tLoss: 0.011709\tF1: 1.0000\tRuntime: 561.2\n",
      "Train Epoch: 7 [140800/269776 (65%)]\tLoss: 0.015888\tF1: 1.0000\tRuntime: 617.0\n",
      "Train Epoch: 7 [153600/269776 (71%)]\tLoss: 0.092678\tF1: 0.9844\tRuntime: 673.0\n",
      "Train Epoch: 7 [166400/269776 (77%)]\tLoss: 0.007016\tF1: 1.0000\tRuntime: 728.9\n",
      "Train Epoch: 7 [179200/269776 (83%)]\tLoss: 0.013652\tF1: 1.0000\tRuntime: 784.8\n",
      "Train Epoch: 7 [192000/269776 (89%)]\tLoss: 0.116155\tF1: 0.9688\tRuntime: 840.7\n",
      "Train Epoch: 7 [204800/269776 (95%)]\tLoss: 0.114296\tF1: 0.9531\tRuntime: 896.7\n",
      "after epoch 7 got f1 score of 0.8633367890995203\n",
      "saved predictions\n",
      "currently best model --> saved\n",
      "{'highest F1': array(0.86333679), 'saved epoch': 7}\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch, start_time = time.time())\n",
    "    f1 = get_mean_F1(model, validation_loader)\n",
    "    print('after epoch {} got f1 score of {}'.format(epoch , f1))\n",
    "    if f1 > info['highest F1']:\n",
    "        info['highest F1'] = np.copy(f1)\n",
    "        info['saved epoch'] = epoch \n",
    "        test(args, model, device, test_loader, epoch, trainDataset, testDataset, path_submission)\n",
    "        torch.save(model, path_model)\n",
    "        print('currently best model --> saved')\n",
    "        \n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save or load outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:37.605441Z",
     "start_time": "2019-08-12T05:02:25.595515Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fieldnames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-81b3a1c49cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m save_output(args, model, device, test_loader, 'CNN2D', \n\u001b[0;32m----> 2\u001b[0;31m             trainDataset, testDataset, path_save = path_submission)\n\u001b[0m",
      "\u001b[0;32m~/challenge/oeawai_challenge/train_utils.py\u001b[0m in \u001b[0;36msave_output\u001b[0;34m(args, model, device, test_loader, which_net, trainDataset, testDataset, path_save)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         writer = csv.DictWriter(writeFile, fieldnames=fieldnames, delimiter=',',\n\u001b[0m\u001b[1;32m     81\u001b[0m                                 quotechar='|', quoting=csv.QUOTE_MINIMAL)\n\u001b[1;32m     82\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fieldnames' is not defined"
     ]
    }
   ],
   "source": [
    "save_output(args, model, device, test_loader, 'CNN2D', \n",
    "            trainDataset, testDataset, path_save = path_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T05:02:22.260740Z",
     "start_time": "2019-08-12T05:02:20.548014Z"
    }
   },
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    #model = torch.load(path_model)\n",
    "    model = torch.load('models/model_7epoch_CNN2D.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# load indices\n",
    "with open(path_submission + \"indices.txt\", \"rb\") as fp:\n",
    "    indices = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-09T07:44:09.139Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "get_mean_F1(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-09T07:44:09.143Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "epoch=10\n",
    "test(args, model, device, test_loader, epoch, trainDataset, testDataset, path_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
